{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensemble Learning & Random Forest","provenance":[],"authorship_tag":"ABX9TyNG+w1EfLRM9y0NCSqw1iRt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"HXIlxrmF8B3D"},"source":["from sklearn.ensemble import RandomForestClassifier \n","from sklearn.ensemble import VotingClassifier \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC \n","\n","log_clf = LogisticRegression()\n","rnd_clf = RandomForestClassifier()\n","svm_clf = SVC()\n","\n","voting_clf = VotingClassifier(\n","    estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svn_clf)],\n","    voting='hard'\n",")\n","voting_clf.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jq0yS8JrUkHS"},"source":["from sklearn.metrics import accuracy_score \n","\n","for clf in (log_clf,rnd_clf,svc_clf,voting_clf):\n","  clf.fit(X_train, y_train)\n","  y_pred = clf.predict(X_test)\n","  print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZA712z_RpE1"},"source":["from sklearn.emsemble import BaggingClassifier \n","from sklearn.tree import DecisionTreeClassifier \n","\n","bag_clf=BaggingClassifier(\n","    DecisionTreeClassifier(),n_estimators=500,\n","    max_samples=100,bootsrap=True,n_jobs=-1\n",")\n","bag_clf.fit(X_train,y_train)\n","y_pred=bag_clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uk-4PelMTnwd"},"source":["from sklearn.ensemble import RandomForestClassifier \n","\n","rnd_clf=RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,n_jobs=-1)\n","rnd_clf.fit(X_train,y_train)\n","\n","y_pred_rf=rnd_clf.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TD4vmJGZnwC"},"source":["from sklearn.ensemble import AdaBoostClassifier \n","\n","ada_clf = AdaBoostClassifier(\n","    DecisionTreeClassifier(max_depth=1),n_estimators=200,\n","    algorithm=\"SAMME.R\",learning_rate=0.5\n",")\n","ada_clf.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YOGMEM-hYl_"},"source":["from sklearn.ensemble import GradientBoostingRegressor \n","\n","gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0)\n","gbrt.fit(X,y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0NwjTjJjEKX"},"source":["#train GBRT ensemble with 120 trees \n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error \n","\n","X_train, X_val, y_train, y_val = train_test_split(X,y)\n","\n","gbrt = GradientBoostingRegressor(max_depth=2,n_estimators=120)\n","gbrt.fit(X_train,y_train)\n","\n","#measure validation error at each stage of training to find optimal number of trees\n","errors = [mean_squared_error(y_val,y_pred)\n","          for y_pred in gbrt.staged_predict(X_val)]\n","bst_n_estimators=np.argmin(errors) + 1 \n","\n","#train another GBRT ensemble using optimal number of trees\n","gbrt_best=GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n","gbrt_best.fit(X_train,y_train)\n","\n","#code stops training when validation error does not improve for 5 iterations in a row \n","gbrt = GradientBoostingRegressor(max_depth=2,warm_start=True)\n","\n","min_val_error = float(\"inf\")\n","error_going_up=0\n","for n_estimators in range(1,120):\n","  gbrt.n_estimators = n_estimators \n","  gbrt,fit(X_train,y_train)\n","  y_pred=gbrt.predict(X_val)\n","  val_error = mean_squared_error(y_val,y_pred)\n","  if val_error < min_val_error:\n","    min_val_error = val_error \n","    error_going_up = 0\n","  else:\n","    error_going_up +=1\n","    if error_going_up = 5:\n","      break"],"execution_count":null,"outputs":[]}]}